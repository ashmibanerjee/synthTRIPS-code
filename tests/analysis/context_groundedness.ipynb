{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys \n",
    "import re\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = ['v', 'p0', 'p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = pd.read_csv(\"../../data/conv-trs/eval/context_retrieval/gemini_retrieved_context.csv\")\n",
    "# llama = pd.read_csv(\"../../data/conv-trs/eval/context_retrieval/llama_retrieved_context.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2302 entries, 0 to 2301\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   config_id             2302 non-null   object\n",
      " 1   original_context      2302 non-null   object\n",
      " 2   cities                2302 non-null   object\n",
      " 3   query_v               2302 non-null   object\n",
      " 4   retrieved_cities_v    2302 non-null   object\n",
      " 5   retrieved_context_v   2302 non-null   object\n",
      " 6   query_p0              2302 non-null   object\n",
      " 7   retrieved_cities_p0   2302 non-null   object\n",
      " 8   retrieved_context_p0  2302 non-null   object\n",
      " 9   query_p1              2302 non-null   object\n",
      " 10  retrieved_cities_p1   2302 non-null   object\n",
      " 11  retrieved_context_p1  2302 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 215.9+ KB\n"
     ]
    }
   ],
   "source": [
    "gemini.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(gt_cities, pred_cities):\n",
    "    return len(set(gt_cities) & set(pred_cities))/len(set(gt_cities))\n",
    "\n",
    "def compute_precision(gt_cities, pred_cities):\n",
    "    return len(set(gt_cities) & set(pred_cities))/len(set(pred_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average recall for method v: 0.796691886431604\n",
      "Average recall for method p0: 0.793619624594313\n",
      "Average recall for method p1: 0.7862845148878322\n"
     ]
    }
   ],
   "source": [
    "def find_city_groundedness(df):\n",
    "    recall = {\n",
    "        'v': [],\n",
    "        'p0': [],\n",
    "        'p1': []\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        for method in METHODS: \n",
    "            recall[method].append(compute_recall(row['cities'], row[f'retrieved_cities_{method}']))\n",
    "\n",
    "    for method in METHODS:\n",
    "        print(f\"Average recall for method {method}: {np.mean(recall[method])}\")\n",
    "\n",
    "# find_city_groundedness(llama)\n",
    "find_city_groundedness(gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(df, col, ref_col):\n",
    "    col_embeddings = model.encode(df[col].tolist(), convert_to_tensor=True)\n",
    "    ref_embeddings = model.encode(df[ref_col].tolist(), convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(col_embeddings, ref_embeddings)\n",
    "    return [similarities[i, i].item() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "# predictions = [\"hello there. what's going on\", \"general kenobi\"]\n",
    "# references = [\"hello there. my name is alice. what's up?\", \"general kenobi\"]\n",
    "# results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert_score(df, ref, pred):\n",
    "    references = df[ref]\n",
    "    predictions = df[pred]  \n",
    "\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang='en')\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(df):\n",
    "    for method in METHODS:\n",
    "        print(f\"BERTScore Results for method {method}\")\n",
    "        res = compute_bert_score(df, 'original_context', f'retrieved_context_{method}')\n",
    "        print(f\"Average Precision: {np.mean(res['precision'])}\")\n",
    "        print(f\"Average Recall: {np.mean(res['recall'])}\")\n",
    "        print(f\"Average F1: {np.mean(res['f1'])}\")\n",
    "        print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Results for method v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.7881949932341157\n",
      "Average Recall: 0.8068414525712293\n",
      "Average F1: 0.7971131069363976\n",
      "-----------------------\n",
      "BERTScore Results for method p0\n",
      "Average Precision: 0.7860391132993143\n",
      "Average Recall: 0.8038690504719131\n",
      "Average F1: 0.7945302904791464\n",
      "-----------------------\n",
      "BERTScore Results for method p1\n",
      "Average Precision: 0.7778071371966295\n",
      "Average Recall: 0.7936860843910329\n",
      "Average F1: 0.7852899350361862\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "# run(llama)\n",
    "run(gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_sim.to_csv(\"../../data/conv-trs/eval/sustainability/gemini_similarity.csv\", index=False)\n",
    "# llama_sim.to_csv(\"../../data/conv-trs/eval/sustainability/llama_similarity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
